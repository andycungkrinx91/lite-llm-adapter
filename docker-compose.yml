version: '3.8'

services:
  backend:
    build: .
    container_name: lite_llm_adapter
    env_file:
      - .env
    environment:
      # Override the REDIS_URL from the .env file for the Docker environment.
      # Inside Docker Compose, services can reach each other using their service name.
      - REDIS_URL=redis://redis:6379
    ports:
      - "8000:8000"
    volumes:
      # Mount the local models directory into the container
      # This avoids including large model files in the image.
      - ./models/gguf_models:/app/models/gguf_models
      # For development, mount the config files to allow changes without rebuilding.
      - ./models/model_config_dev.json:/app/models/model_config_dev.json:ro
      # - ./models/model_config_prod.json:/app/models/model_config_prod.json:ro
      - ./models/model_config_defaults.json:/app/models/model_config_defaults.json:ro
    # For serving LLMs, it's best to use a single worker to avoid duplicating models in memory.
    # Concurrency is handled inside the app with asyncio and the concurrency_manager.
    # We use Gunicorn as a process manager for Uvicorn workers for production-grade stability.
    # -k uvicorn.workers.UvicornWorker: Use Uvicorn's high-performance worker class.
    # -w 1: Use a single worker to avoid duplicating the LLM in memory.
    # --timeout 600: Use a long timeout (10 minutes) to handle slow model loading and long inference tasks.
    # --bind 0.0.0.0:8000: Bind to the correct host and port.
    command: gunicorn -k uvicorn.workers.UvicornWorker -w 1 --timeout 600 --pythonpath /app main:app --bind 0.0.0.0:8000
    depends_on:
      redis:
        condition: service_healthy
    restart: unless-stopped

  redis:
    image: "redis:alpine"
    container_name: redis
    ports:
      - "6379:6379"
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 1s
      timeout: 3s
      retries: 30
    restart: unless-stopped